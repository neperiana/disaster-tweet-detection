{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.data import get_data\n",
    "from src.tuning import parameter_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_sub = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best params for XGBClassifier...\n",
      "No folds = 3\n",
      "\n",
      "Fold 1/3\n",
      "Searching across 96 candidates\n",
      "................................................................................................\n",
      "Fold 2/3\n",
      "Searching across 96 candidates\n",
      "................................................................................................\n",
      "Fold 3/3\n",
      "Searching across 96 candidates\n",
      "................................................................................................"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_features': [400, 500, 600],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [7, 12, 20, 30],\n",
    "    'colsample_bytree': [.75, 1.],\n",
    "}\n",
    "\n",
    "results = parameter_search(\n",
    "    model_class=XGBClassifier,\n",
    "    param_grid=param_grid,\n",
    "    n_splits=3, \n",
    "    X=X,\n",
    "    y=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_features</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>tn</th>\n",
       "      <th>tp</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>600</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.828198</td>\n",
       "      <td>0.501691</td>\n",
       "      <td>0.579893</td>\n",
       "      <td>0.664944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>600</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.824742</td>\n",
       "      <td>0.501998</td>\n",
       "      <td>0.578393</td>\n",
       "      <td>0.663370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>600</td>\n",
       "      <td>300</td>\n",
       "      <td>12</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.818294</td>\n",
       "      <td>0.503524</td>\n",
       "      <td>0.577016</td>\n",
       "      <td>0.660909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>600</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.817604</td>\n",
       "      <td>0.503832</td>\n",
       "      <td>0.576857</td>\n",
       "      <td>0.660718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>600</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.824513</td>\n",
       "      <td>0.499245</td>\n",
       "      <td>0.576229</td>\n",
       "      <td>0.661879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>600</td>\n",
       "      <td>400</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.809772</td>\n",
       "      <td>0.505664</td>\n",
       "      <td>0.575092</td>\n",
       "      <td>0.657718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>600</td>\n",
       "      <td>400</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.499549</td>\n",
       "      <td>0.574952</td>\n",
       "      <td>0.660188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>600</td>\n",
       "      <td>300</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.818754</td>\n",
       "      <td>0.500467</td>\n",
       "      <td>0.574824</td>\n",
       "      <td>0.659610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>600</td>\n",
       "      <td>400</td>\n",
       "      <td>12</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.812997</td>\n",
       "      <td>0.501691</td>\n",
       "      <td>0.573237</td>\n",
       "      <td>0.657344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>600</td>\n",
       "      <td>300</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.829350</td>\n",
       "      <td>0.492519</td>\n",
       "      <td>0.572903</td>\n",
       "      <td>0.660935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_features  n_estimators  max_depth  colsample_bytree        tn  \\\n",
       "74           600           200         12              0.75  0.828198   \n",
       "75           600           200         12              1.00  0.824742   \n",
       "82           600           300         12              0.75  0.818294   \n",
       "76           600           200         20              0.75  0.817604   \n",
       "71           600           100         30              1.00  0.824513   \n",
       "91           600           400         12              1.00  0.809772   \n",
       "88           600           400          7              0.75  0.820828   \n",
       "83           600           300         12              1.00  0.818754   \n",
       "90           600           400         12              0.75  0.812997   \n",
       "80           600           300          7              0.75  0.829350   \n",
       "\n",
       "          tp        f1       auc  \n",
       "74  0.501691  0.579893  0.664944  \n",
       "75  0.501998  0.578393  0.663370  \n",
       "82  0.503524  0.577016  0.660909  \n",
       "76  0.503832  0.576857  0.660718  \n",
       "71  0.499245  0.576229  0.661879  \n",
       "91  0.505664  0.575092  0.657718  \n",
       "88  0.499549  0.574952  0.660188  \n",
       "83  0.500467  0.574824  0.659610  \n",
       "90  0.501691  0.573237  0.657344  \n",
       "80  0.492519  0.572903  0.660935  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='f1', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of target: 0.39166411277965063\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "max_features = 600\n",
    "n_estimators = 200\n",
    "max_depth = 12\n",
    "colsample_bytree = .75\n",
    "\n",
    "# Train with whole model\n",
    "scale_pos_weight = sum(np.where(y==0, 1, 0)) / sum(y)\n",
    "\n",
    "# Extract text features\n",
    "vectorizer = TfidfVectorizer(max_features=max_features) \n",
    "X_dtm = vectorizer.fit_transform(X['clean_text'])\n",
    "X_sub_dtm = vectorizer.transform(X_sub['clean_text'])\n",
    "\n",
    "# Train model\n",
    "clf = XGBClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    colsample_bytree=colsample_bytree,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "clf.fit(X_dtm.toarray(), y)\n",
    "\n",
    "# Get predictions\n",
    "y_sub_pred = clf.predict(X_sub_dtm.toarray())\n",
    "print('Proportion of target:', y_sub_pred.sum()/len(y_sub_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub['target'] = y_sub_pred\n",
    "submission_set = X_sub[['id', 'target']]\n",
    "submission_set.to_csv('../data/submissions/03-xgboost-model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 22.2k/22.2k [00:01<00:00, 13.8kB/s]\n",
      "Successfully submitted to Natural Language Processing with Disaster Tweets"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f ../data/submissions/03-xgboost-model.csv -m \"xgboost tfidf tuned even more\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "128ecb91850c87a80720b6da2917f6766eb045c299b02f7336160a30723bbfad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
