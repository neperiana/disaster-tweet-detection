{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model using sentence-BERT plus keyword vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neperiana/Documents/Projects/disaster_tweet_detection/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/neperiana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/neperiana/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.data import get_data\n",
    "from src.preprocessing import TextToFeatures\n",
    "from src.tuning import parameter_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_sub = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best params for XGBClassifier...\n",
      "No folds = 3\n",
      "\n",
      "Fold 1/3\n",
      "Searching across 18 candidates\n",
      "..................\n",
      "Fold 2/3\n",
      "Searching across 18 candidates\n",
      "..................\n",
      "Fold 3/3\n",
      "Searching across 18 candidates\n",
      ".................."
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    # 'max_features': [500, 600],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 7, 12],\n",
    "    'colsample_bytree': [.75, 1.],\n",
    "}\n",
    "\n",
    "results = parameter_search(\n",
    "    model_class=XGBClassifier,\n",
    "    param_grid=param_grid,\n",
    "    n_splits=3, \n",
    "    vect_type='sentence-BERT',\n",
    "    X=X,\n",
    "    y=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>tn</th>\n",
       "      <th>tp</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>300</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.843626</td>\n",
       "      <td>0.658836</td>\n",
       "      <td>0.705136</td>\n",
       "      <td>0.751231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>300</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.836027</td>\n",
       "      <td>0.663118</td>\n",
       "      <td>0.703998</td>\n",
       "      <td>0.749572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.835797</td>\n",
       "      <td>0.662812</td>\n",
       "      <td>0.703723</td>\n",
       "      <td>0.749304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>300</td>\n",
       "      <td>12</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.843165</td>\n",
       "      <td>0.653027</td>\n",
       "      <td>0.700783</td>\n",
       "      <td>0.748096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.840863</td>\n",
       "      <td>0.654250</td>\n",
       "      <td>0.700719</td>\n",
       "      <td>0.747556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>300</td>\n",
       "      <td>7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.835104</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>0.699764</td>\n",
       "      <td>0.746052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.656696</td>\n",
       "      <td>0.699339</td>\n",
       "      <td>0.745671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.840862</td>\n",
       "      <td>0.650888</td>\n",
       "      <td>0.698136</td>\n",
       "      <td>0.745875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.834183</td>\n",
       "      <td>0.651802</td>\n",
       "      <td>0.695644</td>\n",
       "      <td>0.742993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.771314</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.694322</td>\n",
       "      <td>0.732807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  colsample_bytree        tn        tp        f1  \\\n",
       "17           300         12              1.00  0.843626  0.658836  0.705136   \n",
       "14           300          7              0.75  0.836027  0.663118  0.703998   \n",
       "8            200          7              0.75  0.835797  0.662812  0.703723   \n",
       "16           300         12              0.75  0.843165  0.653027  0.700783   \n",
       "11           200         12              1.00  0.840863  0.654250  0.700719   \n",
       "15           300          7              1.00  0.835104  0.657000  0.699764   \n",
       "2            100          7              0.75  0.834646  0.656696  0.699339   \n",
       "10           200         12              0.75  0.840862  0.650888  0.698136   \n",
       "9            200          7              1.00  0.834183  0.651802  0.695644   \n",
       "1            100          3              1.00  0.771314  0.694300  0.694322   \n",
       "\n",
       "         auc  \n",
       "17  0.751231  \n",
       "14  0.749572  \n",
       "8   0.749304  \n",
       "16  0.748096  \n",
       "11  0.747556  \n",
       "15  0.746052  \n",
       "2   0.745671  \n",
       "10  0.745875  \n",
       "9   0.742993  \n",
       "1   0.732807  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='f1', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of target: 0.3748084584737971\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "n_estimators = 300\n",
    "max_depth = 12\n",
    "colsample_bytree = 1\n",
    "\n",
    "# Train with whole model\n",
    "scale_pos_weight = sum(np.where(y==0, 1, 0)) / sum(y)\n",
    "\n",
    "# Extract text features\n",
    "vectoriser = TextToFeatures(type='sentence-BERT')\n",
    "X_dtm = vectoriser.fit_transform(X['clean_text'])\n",
    "X_sub_dtm = vectoriser.fit_transform(X_sub['clean_text'])\n",
    "\n",
    "# Extract text features - BERT\n",
    "BERT_vectorizer = TextToFeatures(type='sentence-BERT')\n",
    "X_dtm_a = BERT_vectorizer.fit_transform(X['clean_text']).to_numpy()\n",
    "X_sub_dtm_a = BERT_vectorizer.transform(X_sub['clean_text']).to_numpy()\n",
    "\n",
    "# Extract text features - keywords\n",
    "tfidf_vectorizer = TextToFeatures(type='count-vec')\n",
    "X_dtm_b = tfidf_vectorizer.fit_transform(X['keyword'])\n",
    "X_sub_dtm_b = tfidf_vectorizer.transform(X_sub['keyword'])\n",
    "\n",
    "# Join \n",
    "X_dtm = np.concatenate([X_dtm_a, X_dtm_b], axis=1)\n",
    "X_sub_dtm = np.concatenate([X_sub_dtm_a, X_sub_dtm_b], axis=1)\n",
    "\n",
    "\n",
    "# Train model\n",
    "clf = XGBClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    colsample_bytree=colsample_bytree,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "clf.fit(X_dtm, y)\n",
    "\n",
    "# Get predictions\n",
    "y_sub_pred = clf.predict(X_sub_dtm)\n",
    "print('Proportion of target:', y_sub_pred.sum()/len(y_sub_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub['target'] = y_sub_pred\n",
    "submission_set = X_sub[['id', 'target']]\n",
    "submission_set.to_csv('../data/submissions/05-xgboost-model-BERT-plus-keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████████████████████████████████| 22.2k/22.2k [00:01<00:00, 16.1kB/s]\n",
      "Successfully submitted to Natural Language Processing with Disaster Tweets"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f ../data/submissions/05-xgboost-model-BERT-plus-keywords.csv -m \"xgboost BERT plus keywords\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "128ecb91850c87a80720b6da2917f6766eb045c299b02f7336160a30723bbfad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
